{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from nltk import word_tokenize #tokenizer\n",
    "nltk.download('punkt') #sentence tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import html as ihtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_csv = pd.read_csv('train.csv', error_bad_lines=False).fillna(' ')\n",
    "test_csv = pd.read_csv('test.csv', error_bad_lines=False).fillna(' ')\n",
    "display(train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>general_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>007f127033d66db5</td>\n",
       "      <td>\"\\n\\n Link to Sacramento \\n\\nI placed a wikipe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>007f1839ada915e6</td>\n",
       "      <td>Your blatant POV pushing \\n\\nNeither of you gu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>00803f08f55bdcad</td>\n",
       "      <td>\"\\nReliable sources indicate otherwise. Please...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>008198c5a9d85a8e</td>\n",
       "      <td>I did research thank you very much, and it is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>00822d0d01752c7e</td>\n",
       "      <td>\"\\n\\nA \"\"TotallyDisputed\"\" notice would probab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                       comment_text  \\\n",
       "200  007f127033d66db5  \"\\n\\n Link to Sacramento \\n\\nI placed a wikipe...   \n",
       "201  007f1839ada915e6  Your blatant POV pushing \\n\\nNeither of you gu...   \n",
       "202  00803f08f55bdcad  \"\\nReliable sources indicate otherwise. Please...   \n",
       "203  008198c5a9d85a8e  I did research thank you very much, and it is ...   \n",
       "204  00822d0d01752c7e  \"\\n\\nA \"\"TotallyDisputed\"\" notice would probab...   \n",
       "\n",
       "     general_toxic  \n",
       "200              0  \n",
       "201              1  \n",
       "202              0  \n",
       "203              0  \n",
       "204              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# aggregation of toxic labels\n",
    "train = train_csv\n",
    "train['general_toxic'] = ((train['toxic'] + train['severe_toxic'] + train['obscene'] + train['threat'] + train['insult'] + train['identity_hate']) > 0).astype(int)\n",
    "train = train.drop(columns=['toxic', 'severe_toxic', 'severe_toxic', 'threat', 'insult', 'identity_hate', 'obscene'])\n",
    "display(train[200:250].head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate toxic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='general_toxic'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4), dpi=120)\n",
    "train.groupby('general_toxic')['general_toxic'].count().plot(kind='barh', ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "def remove_punc(text):\n",
    "    return ''.join([word for word in text if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    text = BeautifulSoup(ihtml.unescape(text), \"lxml\").text\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "remove_html(train.loc[0, \"comment_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\patrick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://en.wikipedia.org/wiki/Wikipedia_talk:No_original_research/archive15#YouTube_art_as_primary_source\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "c:\\users\\patrick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://finance.yahoo.com/news/7-fascinating-nuggets-another-bewildering-150348488.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "c:\\users\\patrick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://en.wikipedia.org/wiki/Wikipedia:ELYES\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "c:\\users\\patrick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.haaretz.com/news/diplomacy-defense/2-279-calories-per-person-how-israel-made-sure-gaza-didn-t-starve.premium-1.470419\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Explanation Why the edits made under my userna...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \" More I can't make any real suggestions on im...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove HTML\n",
    "train['clean'] = train['comment_text'].apply(lambda x: remove_html(x))\n",
    "train['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>general_toxic</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I cant make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "   general_toxic                                              clean  \n",
       "0              0  Explanation Why the edits made under my userna...  \n",
       "1              0  Daww He matches this background colour Im seem...  \n",
       "2              0  Hey man Im really not trying to edit war Its j...  \n",
       "3              0   More I cant make any real suggestions on impr...  \n",
       "4              0  You sir are my hero Any chance you remember wh...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Punctuations\n",
    "train['clean'] = train['clean'].apply(lambda x: remove_punc(x))\n",
    "#train['clean'].head()\n",
    "display(train.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " #lemmatizer = WordNetLemmatizer() \n",
    " #train['lemmatized'] = train['clean'].apply(lambda x: lemmatize(x))\n",
    " #display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Explanation', 'NN'),\n",
       " ('Why', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('edits', 'NNS'),\n",
       " ('made', 'VBN'),\n",
       " ('under', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('username', 'JJ'),\n",
       " ('Hardcore', 'NNP'),\n",
       " ('Metallica', 'NNP'),\n",
       " ('Fan', 'NNP'),\n",
       " ('were', 'VBD'),\n",
       " ('reverted', 'VBN'),\n",
       " ('They', 'PRP'),\n",
       " ('werent', 'VBP'),\n",
       " ('vandalisms', 'NNS'),\n",
       " ('just', 'RB'),\n",
       " ('closure', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('some', 'DT'),\n",
       " ('GAs', 'NNP'),\n",
       " ('after', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('voted', 'VBD'),\n",
       " ('at', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('Dolls', 'NNP'),\n",
       " ('FAC', 'NNP'),\n",
       " ('And', 'CC'),\n",
       " ('please', 'VB'),\n",
       " ('dont', 'JJ'),\n",
       " ('remove', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('template', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('talk', 'NN'),\n",
       " ('page', 'NN'),\n",
       " ('since', 'IN'),\n",
       " ('Im', 'NNP'),\n",
       " ('retired', 'VBD'),\n",
       " ('now892053827', 'NNS')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = word_tokenize(train['clean'][0])\n",
    "nltk.pos_tag(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using CountVectorizer\n",
    "# pipeline = Pipeline([('vectorizer', CountVectorizer(max_df=0.5,\n",
    "#                                                    min_df=4,\n",
    "#                                                    tokenizer='[a-zA-Z]+')),\n",
    "#                     ('clf', MultinomialNB())], verbose=True)\n",
    "# pipeline.fit(train['comment_text'], train['general_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-cb46c4ac216a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                              \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                              max_features=1000)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvector_space_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# converting the dtype object to unicode string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mn_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_space_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d Total Comments'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mn_comments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#Using TfidVectorizer; COPY PASTED FROM https://www.kaggle.com/piumallick/toxic-comments-sentiment-analysis\n",
    "#for reference only\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                             max_df=0.5,\n",
    "                             min_df=4,\n",
    "                             max_features=1000)\n",
    "vector_space_model = vectorizer.fit_transform(train['comment_text'].values.astype('U').tolist()) # converting the dtype object to unicode string \n",
    "n_comments = vector_space_model.shape[0]\n",
    "print('%d Total Comments' % n_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
